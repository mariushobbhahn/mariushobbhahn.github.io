<hr />
<p>layout: post
title:  "Probabilistic Adjudication: Part I"
date:   2020-01-17 23:01:30 +0200
author:     "Marius Hobbhahn"
category: opinion</p>
<hr />
<h2>What is this mini-series about?</h2>
<p>There are many great workshops on different aspects of judging in debating TODO:refs. However, I think one aspect of the adjudication is neither very prominent nor very well illustrated so far - probabilistic judging. The core thesis is that arguments are not just true or false, bought or not bought in their entirety, but they are 'bought to some degree' or the judges are 'convinced to some extend', i.e. arguments are assigned probabilities or probability distributions. What I want to do in this mini series are two things: a) Set up a basic framework of probabilistic judging and b) integrate and illustrates multiple facettes of debating within that framework. Part I deals with the evaluation of one argument in a vacuum, part II deals with extensions and rebuttal and part III attempts to weight different arguments that aim to prove different metrics. To be very clear - I do not say that this is the ultimately correct way to judge a debate, I want to a) further the discourse on how arguments should be evaluated and b) illustrate how I think about probabilistic judging and give other people who have thought less or different about the issue a possibility to learn or improve my view. Additionally, I want to make this post as understandable as possible such that people who are very new to debating also get a chance to understand it. Given that I have a Machine Learning and Probability Theory background I might find concepts intuitive that you are new to. If such a situation occurs I would be grateful for feedback. If you, on the otherhand, understand what I am saying but disagree with regards to content, I would also be interested in your criticism and possible solutions. </p>
<h2>Part I: evaluating one argument</h2>
<p>In this part I want to focus on evaluating one argument in a vacuum. That means we ignore most of the things happening in a usual debate. We ignore all other teams, all extensions, all rebuttal, all framings, etc. We only consider exactly one argument. The two necessary conditions for an argument to persuade anyone are a) the argument is true and b) the argument is relevant. If you are convinced that an argument is true with high probability but not relevant you very likely do not care a lot about it. If you are convinced that something is important but not that it is true you are not convinced by it either. </p>
<h3>The fundamentals of Probabilistic Inference</h3>
<p>A probability distribution denotes the probability $p(h)$ of a variable $h$ taking a certain value, for example the probability of a human to have a certain height. It is more likely that a person is 1.8m than that they are 1m or 3m tall. It is also possible to condition that variable on another variable, i.e. we could look at the probability distribution of height conditioned on the gender $g$ of a person $p(h | g)$. Even though this is not true in reality, assume for simplicity that there are only two genders. The distribution of the height of all people can suddenly be explained by the two conditional distributions $p(h | g=female)$ and $p(h|g=male)$. Distributions can be defined on different domains. A distribution over probabilities, for example, can only range from 0 to 1 since probabilities below 0 or above 1 are just not defined. The distribution over the height of humans could technically be infinitely large but never below 0. The distribution over the measurement error of a clock could in theory be everywhere between minus and plus infinity, i.e. this clock can show a time that is arbitrarily wrong either in the future or past. To understand what probability distributions, conditionals and domains mean, consider the following figure.</p>
<p>TODO illustrate</p>
<p>Distributions can be used to illustrate a fact about the world as shown in the example of height. However, they can also be used to update our believes given new data. Let's say, for example, your current believe is that you currently think that human height is distributed around the two peaks of 1.6 and 1.7 meters as shown in the above figure. Now you receive new information: The data that we have based that believe on are 10 years old and the decrease in human malnourishment has led to an average growth of humans of 5cm. Therefore you update your believe to a new distribution that is centered around the peaks at 1.65m and 1.75m respectively. There could also be other updates that you make about this believe, i.e. it might be found out that the women whose height was measured during the initial measurements were selected from a particular group in society and therefore bias your result. For example only german women were measured because the study was conducted in germany. Since german women on average are slightly taller than the global average your updated distribution would include this fact and change the peak for women from 1.6m to 1.55m. This notion of updating our believes can be expressed via the fundamental rule of probabilistic inference: Bayes rule.</p>
<p>$$
\underbrace{p(X|Y)}<em>{\text{posterior}} \propto \underbrace{p(Y|X)}</em>{\text{likelihood}} \underbrace{p(X)}_{\text{prior}}
$$</p>
<p>We have a certain prior believe about a thing in the world, we get new data (here referred to as likelihood) and update this believe to yield a posterior. The posterior asks: "what is our believe about variable X after having seen data Y", i.e. what is our updated belief? </p>
<p>I think persuasion can easily be integrated into this model. An adjudicator has the prior believe of the average intelligent globally informed citizen and updates these believes according to the claims made in the debate. As already motivated in the introduction there are two necessary conditions for an argument to be persuasive: it must be true and relevant. I posit that for both, truth and relevance, we have a prior distribution that can be updated through the speakers contributions. The updates should be proportional to the strength of the presented arguments, i.e. a very strong argument must lead to larger updates than a weak one. To illustrate this further consider the following example. </p>
<h2>Example Motion: UBI</h2>
<p>Assume the motion is "THW introduce a universal basic income (UBI)". The model introduced by the goverment is simple: Cut all social benefits, tax the rich and introduce a 1000 dollar per month payment for every person that has citizenship. People without citizenship still get all benefits they have previously gotten. Parents get the money for their children until their 18th birthday. In the following I want to discuss two arguments, a utilitarian calculus and a principle right, that both could be made by a government team. Additionally I want to illustrate how the prior to posterior distribution shift could be viewed for different possible mechanisms and impacts of the respective argument. </p>
<h3>Argument I: a utilitarian calculus</h3>
<p>The statement we assess reads: A UBI improves peoples lives. To evaluate the overall persuasiveness of this argument consider different scenarios. We begin with the update for the distribution of the truth value of this statement. In increasing strength we have</p>
<ol>
<li>An emotional story of a friend that would be helped by this because they could buy themselves a new car. We will call this weak evidence (WE).</li>
<li>Someone pointing out that this has been tested in small scales in some countries with some positive results. However, they do not explain why this experimental setting is transferable to larger society. We will call this mediocre evidence (ME).</li>
<li>A weak mechanism (WM): money is nice for people since it allows them to buy stuff they like and need. </li>
<li>A strong mechanism (SM): For a large group of people it removes the fear and uncertainty of low and unsteady income, i.e. not knowing whether they will be able to pay the rent or go to college. </li>
<li>The weak and strong mechanism are both explained to support the thesis (WM+SM)</li>
</ol>
<p>Note, that I do not say mechanism 4 is necessarily stronger than mechanism 3. Let's for the purpose of this example assume that it had been explained more persuasively (e.g. better analysis, more depth, etc.).</p>
<figure>
  <img src="img/Probabilistic_Judging_1/UBI_truth.png" alt="test"/>
  <figcaption>test</figcaption>
</figure>

<p><img alt="test" src="/img/Probabilistic_Judging_1/UBI_truth.pdf" />{width=100%}</p>
<p><code>{Latex}
\begin{figure}
    \centering
    \includefigure[width=\textwidth]{img/Probabilistic_Judging_1/UBI_truth.pdf}
    \caption{test}
\end{figure}</code></p>
<p><code>{r image-ref-for-in-text, echo = FALSE, message=FALSE, fig.align='center', fig.cap='Some cool caption', out.width='0.75\\linewidth', fig.pos='H'}
knitr::include_graphics("img/Probabilistic_Judging_1/UBI_truth.pdf")</code></p>
<h3>Argument II: a principle right</h3>
<h2>But what about the priors?</h2>
<p>The most used criticism of Bayesian inference is: "Your posterior depends on the prior and if you choose the prior in dumb ways you can produce whatever results you want. Therefore Bayesian Inference does not work." And while it is true in some (but not most) real life applications that it is hard to choose a prior that makes sense we could also choose a prior that just says "I actually really don't know. It could be anything." However, in debating, the average informed voter has a prior believe over a statement. For example if I asked you what your belief about the statement "Donald Trump is currently (January 2020) the president of the united states", then you would be very sure that this is a true and not a false statement. Maybe there would be some lingering uncertainty because he could have been impeached while you were asleep or something but you would be very sure. For the statement "Putin is 20 years old in 2020" your distribution would very much lean to the untrue end. If you were to assess the statement "Brexit will happen" in 2018 the past intelligent voter would have very high uncertainty over the truth of statement probably including all possibilities from false to true. This is also true for the relevance of a given statement. For example, the prior distribution over "the harms of human made climate change" is definitely above zero or the prior distribution over "the wrongness of torturing people for fun without additional reason" is definitely high. </p>
<p>TODO illistrations</p>
<p>All the knowledge that the average intelligent voter has should be seen as prior knowledge in this model of debating. </p>
<p>However, I must point out that debating is a game that implies rules for how the prior distributions are distributed. First, prior knowledge must align with the reality of the AIV, i.e. they know basic facts about the world but are not specialists in any particular field. They know the presidents of most major countries of the world, some basic economics and politics, etc. They do not know the finance minister of the Kongo, the exact melting point of all metals or the exact wording of the law of every country in the world. Even if a particular adjudicator knows these facts, for the purpose of the debate, they must assume not to know any of them. Second, prior knowledge distributions must be easily shiftable, i.e. the adjudicator is easily persuaded by arguments when good reasons are presented. This level of persuadability is probably a lot higher than for the average person that you meet on the street but is part of the game we are playing. </p>
<h2>prior shift vs. absolute value:</h2>
<p>TODO</p>
<h2>Limitations of the model:</h2>
<p>human communication is weird, we don't talk in logic.</p>
<h2>Nerd section: some statistical explanations</h2>
<p><strong><em>One last note:</em></strong></p>
<p>If you have any feedback regarding anything (i.e. layout or opinions) please tell me in a constructive manner via your preferred means of communication.</p>