---
layout:     post
title:      "Sequence Classification using Ensembles of Recurrent Generative Expert Modules"
subtitle:   "Easy-to-understand summary of a paper that we published at ESANN2020"
date:       2020-08-01 20:28:00
author:     "Marius Hobbhahn"
# header-img: "img/Marius2.jpg"
category:   ML_project
tags:       [Machine Learning, Inverse Classification]
---

**Abstract:**

In my first bachelor thesis I tested whether "inverting" the classification process by using gradient information of generative models is effective. One of the problems turned out to be that the latent space became less structured for larger datasets and the results, therefore, got worse. In a follow-up study, which yields the content of this post, we found that using expert modules alleviates this problem and yields better results on sparse datasets. The paper has been accepted for publication at the European Symposium for Artificial Neural Networks (ESANN) but the conference has been postponed to October of 2020 because of Corona. 

<a href='TODO'>Code</a> for this project is publicly available on my github.

## **1. Introduction**

As already discussed in my previous blog posts on Inverse Classification (<a href='http://www.mariushobbhahn.com/2019-10-20-Inverse_Classification_using_generative_models/'>part I</a>, <a href='http://www.mariushobbhahn.com/2020-05-09-Inverse_Classification_with_GANs/'>part II</a>) the Cognitive Modelling group in Tübingen around Martin Butz and Sebastian Otte is very convinced of the hypothesis of <a href='https://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199682737.001.0001/acprof-9780199682737'>The predictive mind</a> (I find it very plausible as well). In short, it states that the mind is to a large extend predictive, i.e. instead of reacting to your surroundings and building your mental model around it your brain predicts it's surroundings and only updates this model slightly through the perceptions you have. The first and second post have focused solely on the predictive part of the brain but this part additionally includes an additional aspect, namely modularity. The brain is not just a random collection of 86 Billion neurons with connections between them but it is highly specialized and modularized. A lot of functionality can be attributed to specific regions of the brain (e.g. vision to the visual cortex) and, thereby, is an interplay between different 'experts'. 

In <a href='http://www.mariushobbhahn.com/2019-10-20-Inverse_Classification_using_generative_models/'>part I</a> we have found that the predictive hypothesis seems to hold very well for small numbers of classes but as soon as you scale up the experiments they tasks seem to get too complex. To contain the complexity we introduced modules/experts for every single class that we want to predict. 

## **2. Setup**

In the standard classification setup you get an input, you forward it through your classification pipeline and you receive a probability vector over class labels. In the Inverse Classification setup used in this post, instead of forwarding the image through the network, you try to generating a similar image from a probability vector. You start with a uniform vector, generate an image, compare this image to the target and backward the loss via any gradient descend method (we chose Adam). The newly updated class probability vector is forwarded again and a new prediction is generated, compared to the target, the loss is backwarded, until, hopefully, you converge to a final probability vector. The general concept is visualized in the following figure. 

<figure>
  <img src="/img/IC_Modular/inverseclassification.pdf"/>
</figure>

The specialty in this setup is that every class has its own expert module instead of one big network for all classes. The experts compete against each other in the sense that they are able or unable to create a sequence that resembles the target and therefore have a higher or lower loss. The competition is won by the network which yields the lowest loss. To clarify consider the following figure. 

<figure>
  <img src="/img/IC_Modular/modularclassification.pdf"/>
</figure>

## **3. Experiments and Results**

While this methodology could be used with any dataset and generative network in theory we have chosen an LSTM since we work with sequential data. The datasets we use are handwritten digit sequences from the UCI repository and MNIST in the sequential interpretation, i.e. instead of interpreting it as a 2D matrix we see it as a sequence of 28-dimensional vectors that are fed into the network sequentially. 
As I have already discussed in the second part, this technique is already very good in the low data regime but does not improve that much with more data. To find out how good exactly it is with few data we have focused our efforts on the low data regime. More specifically this means four data points per class resulting in 40 training points for MNIST and 80 for the handwritten digits. The four training points per class are chosen in two different ways. The first possible setting is one where we try to maximize represenation by the four cluster means of a k-means clustering applied per class. In the second we try to simulate a more realistic setting and choose the data points at random. Each expert learns to generate four different 'types' of a character, i.e. each data point represents its own subclass for every character/number. 

### How good is the Generative Network?

Before we analyse the actual classification results we should investigate how well the invididual expert modules generate their respective letters. If the generation is bad already, Inverse Classification is likely to fail anyways. For this reason we have cherry-picked some generated results to highlight noteworthy things. In the following two figures you can see the four training points in the top row, the targets in the second and the results of the Inverse Classification for that expert in the third. It is important to note that the four sequences in the top are the only data point the network has ever seen. 

<figure>
  <img src="/img/IC_Modular/chars.pdf"/>
</figure>

<figure>
  <img src="/img/IC_Modular/mnist.pdf"/>
</figure>

From these figures we find two important observations: a) The networks are able to recreate/generate sequences they have not seen during training. This becomes especially visible in the third column of the characters figure. Target 3 has a bend in the right-hand tail of the 'a'. This bend can't be found in any of the training points and yet the network is able to create it from gradient information alone. b) The expert modules use combinations of existing knowledge and characters to create these new and previously unknown targets. These combinations might not be interpretable by or feel natural to a human but at least they recreate the desired goal sufficiently well. These two observations are the crucual necessary component for the rest of the experiments as the classification would otherwise just be template matching. 

### How good is Modular Inverse Classification? 

To evaluate our method we have to look at experimental results. Before we do that, though, we test how well 'calibrated' or expert modules are, i.e. how well they are able to classify their own and **only** their own class. If we have a very powerful generative network that can also recreate other classes our entire idea of experts would collapse. On the other hand, we want to make sure that our experts are flexible enough to account for in-class variance, i.e. they are able to recreate different types of the same class. To test this notion of calibration we created heatmaps for both datasets. The heatmaps show the average loss (mean squared error) for all combinations of generators and data points. The cell (0,3) in the top row, for example, shows the loss of the 3-expert for class 0. We apply this procedure for the training data (left) and the test data (right) on MNIST. 

<figure>
  <img src="/img/IC_modular/TODO" width="350"/>
  <img src="/img/IC_modular/TODO" width="350"/>
</figure>

We find that the experts are mostly well calibrated. In the case of the training data the black diagonal indicates that our experts recreate the training data very well while not being able to generate other classes. The same effect can be seen for the test data, even though in slightly weakened fashion - even with the test data (e.g. data that the networks haven't seen before) the training loss for the correct class is always the lowest. 

The figures have given us a rough intuition about the performance of our method so far but we would also like to have some quantifiable comparisons. The experiments conducted in the paper are all in the low-data regime as we have found in <a href='http://www.mariushobbhahn.com/2019-10-20-Inverse_Classification_using_generative_models/'>previous experiments</a> that the Inverse Classification approach is particularly suited for it. We compare the modular approach, or Ensemble of Generative Expert Moduls (EGEM), on two datasets using different methods and setups. The two datasets are MNIST and a set of handwritten letters. Importantly, since we use RNNs, the datasets are represented as sequences not as 2D images. As we are in the low-data regime we only use 4 data points per class. We test two different methods to choose the 4 data points per class: a) randomly ('random') or b) we cluster the data with k-means clustering and choose the cluster representatives ('clustered'). The two conditions are supposed to represent different real-world scenarios. Per dataset and condition we compare five different scenarios: a) EGEM without any addition ('base'), b) EGEM that was trained with the addition of noise ('noise'), c) the best model of the noise scenario ('best'), d) Using a similar LSTM to classify the data in a forward fashion ('forward'), and e) A nearest neighbor approach which is known to yield solid results on small and simple datasets. 

<figure>
  <img src="/img/IC_Modular/TODO.pdf"/>
</figure>



## **4. Conclusions**

**Advantages:** Good in the low data regime

Human interpretability

Problems: scale to larger datasets, 

Potential but moved on. 

This is the last post on the topic of Inverse Classification since I have moved to a different group at the University of Tübingen. Posts of my current work are already in the making. 

#### ***One last note***

If you have any feedback regarding anything (i.e. layout, code, or opinions) please tell me in a constructive manner via your preferred means of communication.

