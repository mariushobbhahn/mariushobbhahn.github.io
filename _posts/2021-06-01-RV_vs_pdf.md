---
layout:     post
title:      "Sums and products of random variables vs. probability density functions"
subtitle:   "A tutorial in pictures"
date:       2021-04-29 20:28:00
author:     "Marius Hobbhahn"
header-img: "img/img-headers/RV_vs_pdf_header.png"
category:   ML_project
tags:       [Machine Learning]
---

## **What is this post about?**

I have been slightly confused about combinations of random variables (RV) in contrast to combinations of probability density functions (pdf). If we have, for example, two Gaussian random variables $$X$$ and $$Y$$ then their sum $$Z=X+Y$$ is again distributed according to a Gaussian. The sum of their pdfs $$p(x) + p(y)$$, however, is __not__ Gaussian. In contrast, the product of two Gaussian random variables $$X \cdot Y$$ is __not__ Gaussian but the product of their pdfs $$p(x) \cdot p(y)$$ is proportional to another Gaussian. 

This post clarifies the differences between sums and products of RVs and pdfs using the example of Gaussians. Since I think in a very visual fashion, most concepts are conveyed using figures.

If you think I got something wrong or suggest feedback just contact me. Check out the accompanying <a href='https://github.com/mariushobbhahn/RV_vs_PDF_Combination'>GitHub repo</a> if you want to play around with the code. 

## Notation

I'll briefly define random variables and probability density functions on a high level. For a more rigorous treatment follow the links in the text. 

A <a href='https://en.wikipedia.org/wiki/Random_variable'>random variable</a> is a function $$X: \Omega \rightarrow E$$ from a set of possible outcomes $$\Omega$$ (e.g. heads or tail) to a <a href='https://en.wikipedia.org/wiki/Measurable_space'>measurable space</a> $$E$$.

A <a href='https://en.wikipedia.org/wiki/Probability_density_function'>probability density function</a> of a continuous random variable is a function that provides a relative likelihood for a given sample from the RV, i.e. it maps from samples to probabilties. 

While the pdf is a function of the RV, we already see that they describe slightly different mathematical concepts. Thus, a priori, it is not surprising that an operation applied to a RV yields a different result as the same operation applied to the associated pdf. 

## Sums

In general, the sum of two RVs from the same distribution don't have to be distributed according to this distribution. The sum of two Gaussian RVs, however, yields another Gaussian while the sum of two Gaussian pdfs does not yield a Gaussian.

<figure>
  <img src="/img/RVs_vs_pdfs/Gaussian_RV_pdf_sum.png"/>
</figure>


To understand why the two images on the right hand side differ we will look at one particular way to derive the sum of RVs.

Let $$X_1$$ and $$X_2$$ be two independent random variables with probability density functions $$f_1(X_1)$$ and $$f_2(X_2)$$. Then we can define a transformation $$g(x_1, x_2)$$ mapping from $$X$$ to a new random variable $$Y$$ with

$$
\begin{aligned}
w &= x_1 + x_2 \\
x_2 &= x_2
\end{aligned}
$$

and inverse transformation $$g^{-1}(w, x_2)$$

$$
\begin{aligned}
x_1 &= w - x_2 \\
x_2 &= x_2
\end{aligned}
$$

With the <a href='https://en.wikipedia.org/wiki/Probability_density_function#Function_of_random_variables_and_change_of_variables_in_the_probability_density_function'>change of variable</a> formula for probability densities we can compute the probability distribution after transformation with 

$$
\begin{aligned}
f_g(w,x_2) &= f(g^{-1}(w,x_2)) \vert J \vert \\
&= f_1(w-x_2) f_2(x_2) \vert J \vert \\
\end{aligned}
$$

where $$\vert J \vert$$ is the determinant of the Jacobian $$\left\vert \frac{\partial}{\partial y} g^{-1}(Y) \right\vert$$, i.e.

$$
\begin{aligned}
J &= \left \vert \begin{pmatrix}
	\frac{\partial x_1}{\partial w} & \frac{\partial x_1}{\partial x_2} \\
	\frac{\partial x_2}{\partial w} & \frac{\partial x_2}{\partial x_2}
\end{pmatrix} \right \vert \\
&= \left \vert \begin{pmatrix}
	1 & -1 \\
	0 & 1
\end{pmatrix} \right \vert
= 1
\end{aligned}
$$

To get the distribution of $$w$$ we have to marginalize out $$x_2$$.

$$
\begin{aligned}
f(w) &= \int_{-\infty}^{+\infty} f_1(w - x_2) f_2(x_2) dx_2 \\
\end{aligned}
$$

This describes the <a href='https://en.wikipedia.org/wiki/Convolution'>Convolution</a> of two pdfs. The convolution of two Gaussian pdfs is another Gaussian pdf with parameters $$\mu = \mu_1 + \mu_2$$ and $$\sigma^2 = \sigma_1^2 + \sigma_2^2$$ (see e.g. <a href='https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables'>this proof</a>). We can visually confirm this result.

![](/img/RVs_vs_pdfs/sum_of_gaussians_convolution.gif)

If you want to step through the GIF at your own pace you can find a HTML version in the <a href='https://github.com/mariushobbhahn/RV_vs_PDF_Combination'>GitHub repo</a> or show me how to embed html files in markdown. 
Using the derivation with the change of variable formula and marginalization, we have a new way to understand and visualize the sum of two Gaussian RVs. 

<figure>
  <img src="/img/RVs_vs_pdfs/Gaussian_joint_sum.png"/>
</figure>

On the left we have the joint distribution $$p(x_1, x_2)$$ of the two independent Gaussian RVs $$X_1$$ and $$X_2$$. Since they are independent, the joint is similar to the product of the marginals $$p(x_1, x_2) = p(x_1) \cdot p(x_2)$$. The marginals are the same distributions as in the first figure. On the right we have the joint distribution after the transformation $$p(w, x_2)$$. We see that the marginal distribution $$p(w)$$ is similar to the histogram of the sum of Gaussian random variables. This shows, that applying the change of variable formula and computing the marginal yields the correct distribution for $$X_1 + X_2$$. 

Now we understand better where the difference between the sum of RVs and sum of pdfs comes from. The sum of RVs is an operation applied to the RVs themselves while the sum of pdfs is applied to a function of the RVs. When formulated as a function of pdfs, the sum of RVs turns out to be their convolution - which is clearly not the sum of pdfs. 

## Products

Similar to the sum, the product of two RVs of the same distribution doesn't have to follow this form again. The product of two Gaussian RVs is not Gaussian but the product of two Gaussian pdfs is proportional to a new Gaussian. 

<figure>
  <img src="/img/RVs_vs_pdfs/Gaussian_RV_pdf_product.png"/>
</figure>

We can see that the two right figures yield different distributions. To understand why, we once again look into how the product of RVs translates into a computation of their pdfs. Similar to the sum case, we have independent RVs $$X_1, X_2$$ with pdfs $$f_1(X_1)$$ and $$f_2(X_2)$$ and a transformation $$h(x_1, x_2)$$ from $$X$$ to $$Y$$. 

$$
\begin{aligned}
z &= x_1 \cdot x_2 \\
x_2 &= x_2
\end{aligned}
$$

and inverse transformation $$h^{-1}(z, x_2)$$

$$
\begin{aligned}
x_1 &= z/x_2 \\
x_2 &= x_2
\end{aligned}
$$

Using the change of variable formula for pdfs we get

$$
\begin{aligned}
f_h(w,X_2) &= f(h^{-1}(z/x_2)) \vert J \vert \\
&= f_1(z/x_2) f_2(x_2) \vert J \vert \\
\end{aligned}
$$

with Jacobian 

$$
\begin{aligned}
J &= \left \vert \begin{pmatrix}
	\frac{\partial x_1}{\partial z} & \frac{\partial x_1}{\partial x_2} \\
	\frac{\partial x_2}{\partial z} & \frac{\partial x_2}{\partial x_2}
\end{pmatrix} \right \vert \\
&= \left \vert \begin{pmatrix}
	\frac{1}{x_2} & -\frac{y}{x_2^2} \\
	0 & 1
\end{pmatrix} \right \vert
= \frac{1}{x_2}
\end{aligned}
$$

Inserting $J$ and marginalizing out $$x_2$$ we get

$$f(z) = \int_0^{+\infty} \frac{1}{x_2} f_1\left(\frac{z}{x_2}\right) f_2(x_2) dx_2$$

which is known as the <a href='https://en.wikipedia.org/wiki/Mellin_transform'>Mellin convolution</a> of $$f_1(x_1)$$ and $$f_2(x_2)$$.

![](/img/RVs_vs_pdfs/product_of_gaussians_convolution.gif)

If you want to step through the GIF at your own pace you can find a HTML version in the <a href='https://github.com/mariushobbhahn/RV_vs_PDF_Combination'>GitHub repo</a>.
Using a change of variable on the joint and then marginalizing we can understand the pdf of the product of two Gaussian RVs visually. 

<figure>
  <img src="/img/RVs_vs_pdfs/Gaussian_joint_product.png"/>
</figure>

On the left we have the joint distribution $$p(x_1, x_2)$$ of the two independent Gaussian RVs $$X_1$$ and $$X_2$$. On the right we have the joint distribution after the transformation $$p(z, x_2)$$. We see that the marginal distribution $$p(z)$$ is similar to the histogram of the product of Gaussian random variables. This shows, that applying the change of variable formula and computing the marginal yields the correct distribution for $$X_1 \cdot X_2$$. 

Similar to the sum we can now clearly see that the product of two independent RVs translates to the Mellin convolution of their pdfs, which is clearly different from the product of pdfs. 

## Conclusion

Applying an operation to a random variable yields a different distribution than applying the same operation to their pdfs. Using the derivations above, we found that the pdf for the result of an operation on RVs can be computed through a change of variables and marginalization. In the cases of independent RVs, the pdfs for sums and products are described by different forms of convolutions. Since convolution is neither equivalent to the sum nor the product of the RVs, we now understand why they yield different results. 

If you have a practical application and don't know which of the two to apply you can think about whether you apply a given operation to a random variable or to their probability density function. For example, the sum of eyes of two dice is an operation which acts on the random variable itself. Bayes rule, on the other hand, describes a product of the probabilities of random variables and is thus an operation between pdfs. 

If you want to understand the math more rigorously, I can recommend <a href='https://www.ime.usp.br/~jmstern/wp-content/uploads/2020/11/Springer1979.pdf'>The Algebra of Random Variables</a>. Even though the book has been published in 1979 already, it still contains all relevant derivations and is easy to read. 

#### ***One last note***

If you want to get informed about new posts you can <a href='http://www.mariushobbhahn.com/subscribe/'>subscribe to my mailing list</a> or <a href='https://twitter.com/MariusHobbhahn'>follow me on Twitter</a>.

If you have any feedback regarding anything (i.e. layout or opinions) please tell me in a constructive manner via your preferred means of communication.


