---
layout:     post
title:      "Sums and products of random variables vs. probability density functions"
subtitle:   "A tutorials in pictures"
date:       2021-03-01 20:28:00
author:     "Marius Hobbhahn"
header-img: "img/img-headers/RV_vs_pdf_header-png"
category:   ML_project
tags:       [Machine Learning]
---

## **What is this post about?**

I have been slightly confused about combinations of random variables (RV) in contrast to combinations of probability density functions (pdf). If we have, for example, two Gaussian random variables $$X$$ and $$Y$$ then their sum $$Z=X+Y$$ is again distributed according to a Gaussian. The sum of their pdfs $$p(x) + p(y)$$, however, is __not__ Gaussian. In contrast, the product of two Gaussian random variables $$X \cdot Y$$ is __not__ Gaussian but the product of their pdfs $$p(x) \cdot p(y)$$ is proportional to another Gaussian. 

This post clarifies the differences between sums and products of RVs and pdfs using the example of Gaussians. Since I think in a very visual fashion, most concepts are conveyed using figures.

If you think I got something wrong or suggest feedback just contact me. Check out the accompanying <a href='https://github.com/mariushobbhahn/RV_vs_PDF_Combination'>GitHub repo</a> if you want to play around with the code. 

## Notation

I'll briefly define random variables and probability density functions on a high level. For a more rigorous treatment follow the links in the text. 

A <a href='https://en.wikipedia.org/wiki/Random_variable'>random variable</a> is a function $$X: \Omega \rightarrow E$$ from a set of possible outcomes $$\Omega$$ (e.g. heads or tail) to a <a href='https://en.wikipedia.org/wiki/Measurable_space'>measurable space</a> $$E$$.

A <a href='https://en.wikipedia.org/wiki/Probability_density_function'>probability density function</a> of a continuous random variable is a function that provides a relative likelihood for a given sample from the RV, i.e. it maps from samples to probabilties. 

While the pdf is a function of the RV, we already see that they describe slightly different mathematical concepts. Thus, a priori, it is not surprising that an operation applied to a RV yields a different result as the same operation applied to the associated pdf. 

## Sums

In general, the sum of two RVs from the same distribution don't have to be distributed according to this distribution. The sum of two Gaussian RVs, however, yields another Gaussian while the sum of two Gaussian pdfs does not yield a Gaussian.

<figure>
  <img src="/img/RVs_vs_pdfs/Gaussian_RV_pdf_sum.png"/>
</figure>


To understand why the two images on the right hand side differ we will look at one particular way to derive the sum of RVs.

Let $$X_1$$ and $$X_2$$ be two independent random variables with probability density functions $$f_1(X_1)$$ and $$f_2(X_2)$$. Then we can define a transformation $$g(x_1, x_2)$$ mapping from $$X$$ to a new random variable $$Y$$ with

$$
\begin{aligned}
w &= x_1 + x_2 \\
x_2 &= x_2
\end{aligned}
$$

and inverse transformation $$g^{-1}(w, x_2)$$

$$
\begin{aligned}
x_1 &= w - x_2 \\
x_2 &= x_2
\end{aligned}
$$

With the <a href='https://en.wikipedia.org/wiki/Probability_density_function#Function_of_random_variables_and_change_of_variables_in_the_probability_density_function'>change of variable</a> formula for probability densities we can compute the probability distribution after transformation with 

$$
\begin{aligned}
f_g(w,x_2) &= f(g^{-1}(w,x_2)) \vert J \vert \\
&= f_1(w-x_2) f_2(x_2) \vert J \vert \\
\end{aligned}
$$

where $$\vert J \vert$$ is the determinant of the Jacobian $$\left\vert \frac{\partial}{\partial y} g^{-1}(Y) \right\vert$$, i.e.

$$
\begin{aligned}
J &= \left \vert \begin{pmatrix}
	\frac{\partial x_1}{\partial w} & \frac{\partial x_1}{\partial x_2} \\
	\frac{\partial x_2}{\partial w} & \frac{\partial x_2}{\partial x_2}
\end{pmatrix} \right \vert \\
&= \left \vert \begin{pmatrix}
	1 & -1 \\
	0 & 1
\end{pmatrix} \right \vert
= 1
\end{aligned}
$$

To get the distribution of $$w$$ we have to marginalize out $$x_2$$.

$$
\begin{aligned}
f(w) &= \int_{-\infty}^{+\infty} f_1(w - x_2) f_2(x_2) dx_2 \\
\end{aligned}
$$

This describes the <a href='https://en.wikipedia.org/wiki/Convolution'>Convolution</a> of two pdfs. The convolution of two Gaussian pdfs is another Gaussian pdf with parameters $$\mu = \mu_1 + \mu_2$$ and $$\sigma^2 = \sigma_1^2 + \sigma_2^2$$ (see e.g. <a href='https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables'>this proof</a>). We can visually confirm this result.

![](/img/RVs_vs_pdfs/sum_of_gaussians_convolution.gif)

Using the derivation with the change of variable formula and marginalization, we have a new way to understand and visualize the sum of two Gaussian RVs.


<figure>
  <img src="/img/RVs_vs_pdfs/Gaussian_joint_sum.png"/>
</figure>

TODO: describe figure

Now we know where the difference between sum of RVs and sum of pdfs comes from. The sum of RVs is an operation directly applied to the RVs themselves while the sum of pdfs is applied to a function of the RVs. Sort of in a different space. When we translate the sum of RVs to the space of pdfs, it becomes the convolution of pdfs. 

TODO: rewrite above

## Products

Similar to the sum, the product of two RVs of the same distribution doesn't have to follow this form again. The product of two Gaussian RVs is not Gaussian but the product of two Gaussian pdfs is proportional to a new Gaussian. 

<figure>
  <img src="/img/RVs_vs_pdfs/Gaussian_RV_pdf_product.png"/>
</figure>

We can see that the two right figures yield different distributions. To understand why, we once again look into how the product of RVs translates into a computation of their pdfs. Similar to the sum case, we have independent RVs $$X_1, X_2$$ with pdfs $$f_1(X_1)$$ and $$f_2(X_2)$$ and a transformation $$h(x_1, x_2)$$ from $$X$$ to $$Y$$. 

$$
\begin{aligned}
z &= x_1 \cdot x_2 \\
x_2 &= x_2
\end{aligned}
$$

and inverse transformation $$h^{-1}(z, x_2)$$

$$
\begin{aligned}
x_1 &= z/x_2 \\
x_2 &= x_2
\end{aligned}
$$

Using the change of variable formula for pdfs we get

$$
\begin{aligned}
f_h(w,X_2) &= f(h^{-1}(z/x_2)) \vert J \vert \\
&= f_1(z/x_2) f_2(x_2) \vert J \vert \\
\end{aligned}
$$

with Jacobian 

$$
\begin{aligned}
J &= \left \vert \begin{pmatrix}
	\frac{\partial x_1}{\partial z} & \frac{\partial x_1}{\partial x_2} \\
	\frac{\partial x_2}{\partial z} & \frac{\partial x_2}{\partial x_2}
\end{pmatrix} \right \vert \\
&= \left \vert \begin{pmatrix}
	\frac{1}{x_2} & -\frac{y}{x_2^2} \\
	0 & 1
\end{pmatrix} \right \vert
= \frac{1}{x_2}
\end{aligned}
$$

Inserting $J$ and marginalizing out $$x_2$$ we get

$$f(z) = \int_0^{+\infty} \frac{1}{x_2} f_1\left(\frac{z}{x_2}\right) f_2(x_2) dx_2$$

which is known as the <a href='https://en.wikipedia.org/wiki/Mellin_transform'>Mellin convolution</a> of $$f_1(x_1)$$ and $$f_2(x_2)$$.

![](/img/RVs_vs_pdfs/product_of_gaussians_convolution.gif)

Using a change of variable on the joint and then marginalizing we can understand the pdf of the product of two Gaussian RVs visually.

<figure>
  <img src="/img/RVs_vs_pdfs/Gaussian_joint_product.png"/>
</figure>

TODO: explain figure.

## Conclusion

depends on what you actually want compute; TODO: explain how to find out?

TODO: sketch of different 

If you want to understand the math more rigorously, I can recommend TODO: link book. Even though it has been published in 1979 already, it is still easy to understand.

#### ***One last note***

If you want to get informed about new posts you can <a href='http://www.mariushobbhahn.com/subscribe/'>subscribe to my mailing list</a> or <a href='https://twitter.com/MariusHobbhahn'>follow me on Twitter</a>.

If you have any feedback regarding anything (i.e. layout or opinions) please tell me in a constructive manner via your preferred means of communication.


