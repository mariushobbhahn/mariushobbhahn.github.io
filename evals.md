---
layout: page
title: Evals
subtitle: A list of evals resources
---

This page is work in progress.

I want to grow the evals field and make it more accessible for people who want to start doing evals. My goal is that someone who is new to evals could either spend a day, a week or a month on the materials here and gain the most important insights and skills for evals for that timeframe. While the materials lean towards the applied side of evals, I also want someone with a non-technical background to benefit from them. 

I think evals is extremely accessible as a field and very friendly to newcomers. If you already have a lot of research and software engineering experience, you will be able to become very good at evals very quickly. If you are a newcomer, evals allow you to get started with a minimal amount of background knowledge. Because the field is so empirical and hands-on, you can iterate very quickly and thus get a lot of direct feedback immediately.

I intend to update this page from time to time. Feel free to reach out with resources that you think should be included. 

#### A starter Guide to Evals

We have written [a starter guide to evals](https://www.apolloresearch.ai/blog/a-starter-guide-for-evals) as a general introduction to the field and suggestions on how to get started. If you prefer a presentation over text, you can watch [this talk](https://www.youtube.com/watch?v=zMmJEOl1Cco) I gave for the Talos fellowship. 

In general, because evals is such an empirical field, I would recommend to get your hands dirty as soon as possible, as described at the bottom of the [starter guide](https://www.apolloresearch.ai/blog/a-starter-guide-for-evals).

#### Inspect

[Inspect](https://inspect.ai-safety-institute.org.uk/) is an open source evals library designed and maintained by UK AISI and spearheaded by [JJ Allaire](https://en.wikipedia.org/wiki/Joseph_J._Allaire), who intends to develop and support the framework for many years. It supports a wide range of evals and is fairly easy to use. It is already used by a large number of people in the evals community and I think it will become the defacto standard tool for evals similar to how PyTorch is the defacto standard for machine learning. 

At Apollo, we have also decided to switch to using inspect for our evals (see details here TODO).

I think there is a lot of room for people to contribute to Inspect, e.g. I think there could be much better tutorials. I also think it might be great for people who want to work on evals as software engineers to start contributing to the library as a way to test fit and make a name for themselves. 

#### The ARENA evals materials

I worked with Chloe and James (they did most of the heavy lifting) to develop [evals materials for ARENA](https://arena3-chapter3-llm-evals.streamlit.app/). The materials are designed for a full week of training and aim to give you some basic skills for evals.

This was the first iteration, so there are still a lot of things we want to improve. We're keen on feedback from people who have either done ARENA or go through the materials in their free time. 

#### The evals gap

TODO

TODO include image.

#### An opinionated Evals Reading list

We have recently written a detailed [opinionated evals reading list](https://www.apolloresearch.ai/blog/an-opinionated-evals-reading-list). Our goal was that people can spend little time and get a good overview of the most important work in evals or that they can spend a lot of time and understand everything in detail. 

If you understand all the papers we listed as "core" well, I would claim that you know about 90% of the important ideas in evals (at least the ones that are publicly available; there is, of course, a lot of latent knowledge that hasn't been written up yet). 

## Future plans and missing resources

- Evals slack
- Evals list of open problems 
- Better Inspect tutorials
- Evals cookbook
- More "day-to-day" evals resources, e.g. a video of someone building an eval.

If you're keen to be involved in any of the above, please reach out. 